{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47173716",
   "metadata": {},
   "source": [
    "# Algoritmo do Bandido\n",
    "\n",
    "<img src=\"k-armed_bandit.png\" width=\"400\">\n",
    "\n",
    "Você deve repetidamente escolher uma entre k diferentes ações. Após cada escolha, você recebe uma recompensa numérica (distribuição probabilística) ligada a sua ação e que pode influenciar suas escolhas futuras. Seu objetivo é maximizar a recompensa total esperada ao final de um período fazendo as escolhas certas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc300cb",
   "metadata": {},
   "source": [
    "Importando bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab724d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Introdução ao Aprendizado por Reforço - PPGEE\n",
    "# Prof. Armando Alves Neto\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d94b5",
   "metadata": {},
   "source": [
    "Criando a classe do Bandido.\n",
    "\n",
    "A função ```selectAction()``` implementa duas versões de escolha da ação, a quase-gulosa (ou $\\varepsilon$-gulosa),\n",
    " \n",
    "$$\n",
    "A_t = \n",
    "\\begin{cases}\n",
    "    \\arg\\!\\max\\limits_{a} ~Q_t(a) & \\text{com probabilidade}~ 1 - \\varepsilon,\\\\\n",
    "    \\textrm{ação aleatória} & \\text{com probabilidade}~ \\varepsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "e a Upper-Confidence-Bound (UCB),\n",
    "\n",
    "$$\n",
    "A_t \\doteq \\arg\\!\\max_{a} \\left[ Q_t(a) + c~ \\sqrt{\\frac{\\ln t}{N_t(a)}}~\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# k-armed bandit\n",
    "##########################################\n",
    "class Bandit:\n",
    "    ##########################################\n",
    "    def __init__(self, K, steps, eps=0.0, method='e-greedy'):\n",
    "\n",
    "        # k bandits\n",
    "        self.K = K\n",
    "        # testbed\n",
    "        self.testbed = [{'mean': 2.0*np.random.uniform(-1.0,1.0), 'std': 0.5} for a in range(K)]\n",
    "        # numero de jogadas de 1 episodio\n",
    "        self.steps = steps\n",
    "        # epsilon-greedy\n",
    "        self.eps = eps\n",
    "        # constant c do UCB\n",
    "        self.c = parameters['c']\n",
    "\n",
    "        # initialize action values\n",
    "        self.Q = np.array([0.0 for a in range(self.K)])\n",
    "        self.N = np.array([0.0 for a in range(self.K)])\n",
    "\n",
    "        # metodo\n",
    "        self.method = method\n",
    "        # instante de tempo\n",
    "        self.t = 0\n",
    "        # melhor acao dentre todas\n",
    "        self.best_action = np.argmax([t['mean'] for t in self.testbed])\n",
    "\n",
    "    ##########################################\n",
    "    # e-greedy action selection\n",
    "    def selectAction(self):\n",
    "\n",
    "        ##########################################\n",
    "        if self.method == 'e-greedy':\n",
    "            # exploration\n",
    "            if np.random.random() <= self.eps:\n",
    "                a = np.random.choice(self.K)\n",
    "            # explotation\n",
    "            else:\n",
    "                a = np.argmax(self.Q)\n",
    "\n",
    "        ##########################################\n",
    "        if self.method == 'ucb':\n",
    "            ucb = np.array([self.c*np.sqrt(np.log(self.t)/(self.N[A]+0.01)) for A in range(self.K)])\n",
    "            a = np.argmax(self.Q + ucb)\n",
    "        return a\n",
    "\n",
    "    ##########################################\n",
    "    # bandit\n",
    "    def bandit(self, A):\n",
    "        return np.random.normal(self.testbed[A]['mean'], self.testbed[A]['std'])\n",
    "\n",
    "    ##########################################\n",
    "    # episode\n",
    "    def runEpisode(self):\n",
    "        # rewards de um epsodio\n",
    "        rewards = []\n",
    "        # escolha da melhor acao\n",
    "        best_action = []\n",
    "\n",
    "        # main loop\n",
    "        for i in range(self.steps):\n",
    "\n",
    "            self.t += 1\n",
    "            # select action\n",
    "            A = self.selectAction()\n",
    "\n",
    "            # get reward\n",
    "            R = self.bandit(A)\n",
    "\n",
    "            # update action-value\n",
    "            self.N[A] = self.N[A] + 1\n",
    "            self.Q[A] = self.Q[A] + (1.0/self.N[A])*(R - self.Q[A])\n",
    "\n",
    "            # accumulate rewards\n",
    "            rewards.append(R)\n",
    "            # acumula melhor acao\n",
    "            best_action.append(A == self.best_action)\n",
    "\n",
    "        return rewards, best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f343b76",
   "metadata": {},
   "source": [
    "Aqui, com os parâmetros fornecidos, rodamos vários episódios do problema, retornando o comportamento médio destes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# tests\n",
    "##########################################\n",
    "def main(parameters):\n",
    "\n",
    "    accum_reward = []\n",
    "    accum_action = []\n",
    "    for i in range(parameters['episodes']):\n",
    "        # cria o testbed\n",
    "        bandit = Bandit(K=parameters['k-armed'], steps=parameters['steps'], eps=parameters['eps'], method=parameters['method'])\n",
    "        \n",
    "        # roda um epsodio\n",
    "        rewards, best_action = bandit.runEpisode()\n",
    "        accum_reward.append(rewards)\n",
    "        accum_action.append(best_action)\n",
    "\n",
    "    # media de todos os epsodios\n",
    "    avg_reward = np.mean(accum_reward, 0)\n",
    "\n",
    "    # acao otima\n",
    "    opt_action = 100.0*np.mean(accum_action, 0)\n",
    "    \n",
    "    return avg_reward, opt_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c18c0",
   "metadata": {},
   "source": [
    "Definindo parâmetros principais do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c24177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sns.set()\n",
    "    \n",
    "    # parametros\n",
    "    parameters = {\n",
    "                'k-armed'  : 10,\n",
    "                'steps'    : 1000,\n",
    "                'episodes' : 2000,\n",
    "                'method'   : '',\n",
    "                'eps'      : 0.0,\n",
    "                'c'        : 1.0,\n",
    "            }\n",
    "    \n",
    "    ##########################################\n",
    "    # greedy and nongreedy\n",
    "    parameters['method'] = 'e-greedy'\n",
    "    for eps in [0.0, 0.01, 0.1]:\n",
    "        parameters['eps'] = eps\n",
    "        avg_reward, opt_action = main(parameters)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        plt.plot(avg_reward, label='E-greedy (e=%.2f)'%eps)\n",
    "        plt.figure(2)\n",
    "        plt.plot(opt_action, label='E-greedy (e=%.2f)'%eps)\n",
    "    \n",
    "    ##########################################\n",
    "    # Upper-Confidence-Bound\n",
    "    parameters['method'] = 'ucb'\n",
    "    for c in [1.0, 2.0]:\n",
    "        parameters['c'] = c\n",
    "        avg_reward, opt_action = main(parameters)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot(avg_reward, label='UCB (c=%.2f)'%c)\n",
    "        plt.figure(2)\n",
    "        plt.plot(opt_action, label='UCB (c=%.2f)'%c)\n",
    "    \n",
    "    ##########################################\n",
    "    plt.figure(1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average rewards')\n",
    "    plt.legend(loc='lower right', fancybox=True, shadow=True, fontsize=12, facecolor='w')\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Optimal action [%]')\n",
    "    plt.legend(loc='lower right', fancybox=True, shadow=True, fontsize=12, facecolor='w')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
